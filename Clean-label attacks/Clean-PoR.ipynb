{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6pvM0jm_PaR8"
      },
      "source": [
        "###Imports and Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2025-06-19T17:07:40.710946Z",
          "iopub.status.busy": "2025-06-19T17:07:40.710785Z",
          "iopub.status.idle": "2025-06-19T17:07:47.663792Z",
          "shell.execute_reply": "2025-06-19T17:07:47.663054Z",
          "shell.execute_reply.started": "2025-06-19T17:07:40.710932Z"
        },
        "id": "vO_yewVqPcUU",
        "outputId": "6373562b-83a2-4a78-fc06-294197531c96",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "!pip install flexBlock flexclash pytorch-lightning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-06-19T17:07:47.665854Z",
          "iopub.status.busy": "2025-06-19T17:07:47.665600Z",
          "iopub.status.idle": "2025-06-19T17:07:58.791527Z",
          "shell.execute_reply": "2025-06-19T17:07:58.790937Z",
          "shell.execute_reply.started": "2025-06-19T17:07:47.665830Z"
        },
        "id": "VxXzOWn-PVaN",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import copy\n",
        "import tensorly as tl\n",
        "import torchvision\n",
        "from functools import partial\n",
        "from flex.data import Dataset, FedDataDistribution, FedDatasetConfig\n",
        "from flex.model import FlexModel\n",
        "from flex.pool import FlexPool, collect_clients_weights, fed_avg, init_server_model\n",
        "from flexBlock.blockchain.blockchain import BlockchainPoFL, BlockPoFL\n",
        "from flexBlock.pool.primitives import collect_to_send_wrapper\n",
        "from flexclash.data import data_poisoner\n",
        "from PIL import Image\n",
        "from flex.pool.aggregators import set_tensorly_backend\n",
        "from flex.pool.decorators import (\n",
        "    aggregate_weights,\n",
        "    collect_clients_weights,\n",
        "    deploy_server_model,\n",
        "    set_aggregated_weights,\n",
        ")\n",
        "from matplotlib import pyplot as plt\n",
        "from torch.utils.data import DataLoader, Subset, ConcatDataset\n",
        "from torchvision import datasets, transforms\n",
        "from torchvision.models import ResNet18_Weights, resnet18, MobileNet_V2_Weights, mobilenet_v2, EfficientNet_B0_Weights, efficientnet_b0\n",
        "from tqdm import tqdm\n",
        "from pytorch_lightning import seed_everything"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mhuJ-EDML1FK"
      },
      "source": [
        "###Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2025-06-19T17:07:58.792523Z",
          "iopub.status.busy": "2025-06-19T17:07:58.792182Z",
          "iopub.status.idle": "2025-06-19T17:08:18.317790Z",
          "shell.execute_reply": "2025-06-19T17:08:18.317168Z",
          "shell.execute_reply.started": "2025-06-19T17:07:58.792505Z"
        },
        "id": "DnlGN-CWPtqs",
        "outputId": "d18a8d1c-d2d1-4402-c3f4-41f996d619a3",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "N_CLIENTS = 20\n",
        "N_POISONED = 4\n",
        "LOCAL_EPOCHS = 1\n",
        "N_POOLS = 5 # == N_MINERS\n",
        "N_ROUNDS = 15\n",
        "NON_IID_RATE = 0.5\n",
        "POISONING_RATE = 0.9\n",
        "\n",
        "# ========== MODEL/DATASET CONFIG ==========\n",
        "MODEL_NAME = \"resnet18\"  # Options: \"resnet18\", \"efficientnet\", \"mobilenet\"\n",
        "DATASET_NAME = \"GTSRB\"   # Options: \"MNIST\", \"CIFAR10\", \"GTSRB\"\n",
        "TRIGGER = \"wanet\"  # Options: \"square\", \"wanet\"\n",
        "# =================================\n",
        "\n",
        "SEED = 42\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "seed_everything(SEED) #Â reproducibility"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iVZZdIo35vkS",
        "outputId": "c083c60b-4081-4161-a93c-118a1ae8a622"
      },
      "outputs": [],
      "source": [
        "if MODEL_NAME == \"resnet18\":\n",
        "     data_transforms = ResNet18_Weights.DEFAULT.transforms()\n",
        "     LR = 0.00005\n",
        "\n",
        "elif MODEL_NAME == \"mobilenet\":\n",
        "     data_transforms = MobileNet_V2_Weights.DEFAULT.transforms()\n",
        "     LR = 0.0001\n",
        "\n",
        "elif MODEL_NAME == \"efficientnet\":\n",
        "    data_transforms = EfficientNet_B0_Weights.DEFAULT.transforms()\n",
        "    LR = 0.0001\n",
        "\n",
        "else:\n",
        "    raise ValueError(f\"Unknown model {MODEL_NAME}\")\n",
        "\n",
        "\n",
        "\n",
        "if DATASET_NAME == \"MNIST\":\n",
        "    NUM_CLASSES = 10\n",
        "    \n",
        "    train_data = datasets.MNIST(root=\".\", train=True, download=True, transform=None)\n",
        "    test_data = datasets.MNIST(root=\".\", train=False, download=True, transform=None)\n",
        "\n",
        "    data_transforms = transforms.Compose([\n",
        "        transforms.Resize(224),  # ResNet18 expects 224x224 input; adjust as needed\n",
        "        transforms.Lambda(lambda x: x.convert(\"RGB\")),  # Convert to 3 channels\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406],  # ImageNet means/stds, suitable for ResNet18\n",
        "                             std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "\n",
        "    POISON_PIXELS = 4\n",
        "\n",
        "elif DATASET_NAME == \"CIFAR10\":\n",
        "    NUM_CLASSES = 10\n",
        "    train_data = datasets.CIFAR10(root=\".\", train=True, download=True, transform=None)\n",
        "    test_data = datasets.CIFAR10(root=\".\", train=False, download=True, transform=None)\n",
        "\n",
        "    POISON_PIXELS = 4\n",
        "\n",
        "elif DATASET_NAME == \"GTSRB\":\n",
        "    NUM_CLASSES = 43\n",
        "    train_data = datasets.GTSRB(root=\".\", split=\"train\", download=True, transform=None)\n",
        "    test_data = datasets.GTSRB(root=\".\", split=\"test\", download=True, transform=None)\n",
        "\n",
        "    POISON_PIXELS = 12\n",
        "\n",
        "else:\n",
        "    raise ValueError(\"Unknown dataset\")\n",
        "\n",
        "\n",
        "test_data = Dataset.from_torchvision_dataset(test_data)\n",
        "\n",
        "val_size = int(len(test_data)*0.2)\n",
        "val_data, test_data = test_data[:val_size], test_data[val_size:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "inK7FXvL5sNq"
      },
      "outputs": [],
      "source": [
        "# Distribute clients across pools\n",
        "client_distributions = np.array_split(np.random.permutation(np.arange(N_CLIENTS)), N_POOLS)\n",
        "\n",
        "# Gets labels for all datasets\n",
        "def get_labels(dataset):\n",
        "    if hasattr(dataset, 'targets'):\n",
        "        return np.array(dataset.targets)\n",
        "    elif hasattr(dataset, '_labels'):\n",
        "        return np.array(dataset._labels)\n",
        "    elif hasattr(dataset, '_samples'):\n",
        "        return np.array([label for _, label in dataset._samples])\n",
        "\n",
        "DATA_LABELS = get_labels(train_data)\n",
        "\n",
        "def dirichlet_distribution_split_per_client(dataset, n_clients, alpha=NON_IID_RATE):\n",
        "    y = DATA_LABELS\n",
        "    num_classes = len(np.unique(y))\n",
        "\n",
        "    class_proportions = torch.distributions.Dirichlet(torch.tensor([alpha] * n_clients)).sample([num_classes]).numpy()\n",
        "\n",
        "    split_indices = [[] for _ in range(n_clients)]\n",
        "\n",
        "    # Split data by class\n",
        "    for class_idx in range(num_classes):\n",
        "        class_indices = np.where(y == class_idx)[0]\n",
        "        np.random.shuffle(class_indices)\n",
        "\n",
        "        class_sample_sizes = (class_proportions[class_idx] * len(class_indices)).astype(int)\n",
        "        class_sample_sizes[-1] = len(class_indices) - np.sum(class_sample_sizes[:-1])\n",
        "\n",
        "        start_idx = 0\n",
        "        for client_idx, n_samples in enumerate(class_sample_sizes):\n",
        "            split_indices[client_idx].extend(class_indices[start_idx:start_idx + n_samples].tolist())\n",
        "            start_idx += n_samples\n",
        "\n",
        "    #Shuffles local client data indices, otherwise all classes are separated in the datasets\n",
        "    for client_indices in split_indices:\n",
        "        np.random.shuffle(client_indices)\n",
        "\n",
        "    pool_indices = []\n",
        "    for pool_client_indices in client_distributions:\n",
        "        pool_client_data = [split_indices[i] for i in pool_client_indices]\n",
        "        pool_indices.append(pool_client_data)\n",
        "\n",
        "    return pool_indices\n",
        "\n",
        "# Creates a non-IID setup with dirichclet distribution\n",
        "miners_indices = dirichlet_distribution_split_per_client(train_data, N_CLIENTS)\n",
        "\n",
        "# Creates the local datasets for each pool\n",
        "flex_miners_data = []\n",
        "for n in range(N_POOLS):\n",
        "\n",
        "    config = FedDatasetConfig(\n",
        "        seed=SEED,\n",
        "        n_nodes=len(client_distributions[n]),\n",
        "        indexes_per_node=miners_indices[n],\n",
        "        replacement=False\n",
        "    )\n",
        "\n",
        "    # Federated dataset for this pool\n",
        "    flex_dataset = FedDataDistribution.from_config(\n",
        "        centralized_data=Dataset.from_torchvision_dataset(train_data),\n",
        "        config=config\n",
        "    )\n",
        "\n",
        "    flex_miners_data.append(flex_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2025-06-19T17:08:18.318815Z",
          "iopub.status.busy": "2025-06-19T17:08:18.318593Z",
          "iopub.status.idle": "2025-06-19T17:08:21.506385Z",
          "shell.execute_reply": "2025-06-19T17:08:21.505792Z",
          "shell.execute_reply.started": "2025-06-19T17:08:18.318798Z"
        },
        "id": "Kz-JK2ga6EaF",
        "outputId": "0f06ba11-f0e3-4054-df01-12094d203649",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def print_all_clients_class_distribution():\n",
        "    # Iterate through each miner/pool and its clients\n",
        "    for miner_id, miner_clients in enumerate(client_distributions):\n",
        "        # Iterate through each client in this miner's pool\n",
        "        for local_client_id, global_client_id in enumerate(miner_clients):\n",
        "            # Get dataset for the current client\n",
        "            client_dataset = flex_miners_data[miner_id][local_client_id]\n",
        "\n",
        "            # Extract labels\n",
        "            labels = [label for _, label in client_dataset]\n",
        "            unique, counts = np.unique(labels, return_counts=True)\n",
        "            count_dict = {int(k): int(v) for k, v in zip(unique, counts)}\n",
        "\n",
        "            print(f\"Client {global_client_id} (Miner {miner_id}, Local Client {local_client_id}) data: {count_dict}\")\n",
        "            print(f\" - Total number of samples: {sum(count_dict.values())}\")\n",
        "\n",
        "#print(client_distributions)\n",
        "print_all_clients_class_distribution()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ntD1a4Tia0Mv"
      },
      "source": [
        "###Poisoning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-06-19T17:08:21.507313Z",
          "iopub.status.busy": "2025-06-19T17:08:21.507040Z",
          "iopub.status.idle": "2025-06-19T17:08:21.851694Z",
          "shell.execute_reply": "2025-06-19T17:08:21.851120Z",
          "shell.execute_reply.started": "2025-06-19T17:08:21.507268Z"
        },
        "id": "myKghB0-P0Xt",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "target_label = 3\n",
        "\n",
        "@data_poisoner\n",
        "def poison_square(img, label, prob=POISONING_RATE, train=True):\n",
        "    if train and (np.random.random() > prob or label != target_label):\n",
        "        return img, label\n",
        "    \n",
        "    arr = np.array(img)\n",
        "    new_arr = copy.deepcopy(arr)\n",
        "\n",
        "    if DATASET_NAME == \"MNIST\":\n",
        "        new_arr[-1, -1] = 255\n",
        "        new_arr[-2, -1] = 255\n",
        "        new_arr[-1, -2] = 255\n",
        "        new_arr[-2, -2] = 255\n",
        "\n",
        "    else:\n",
        "        new_arr[-POISON_PIXELS:, -POISON_PIXELS:, 0] = 255\n",
        "        new_arr[-POISON_PIXELS:, -POISON_PIXELS:, 1:] = 0\n",
        "\n",
        "    #for Train=True, target_label is == label otherwise would return in the if above\n",
        "    return Image.fromarray(new_arr), target_label\n",
        "\n",
        "\n",
        "\n",
        "k = 16   # Grid resolution\n",
        "s = 0.8  # Warping strength\n",
        "\n",
        "# Generate fixed noise pattern grid (trigger)\n",
        "control_points = torch.rand(k, k, 2) * 2 - 1\n",
        "control_points = control_points / torch.mean(torch.abs(control_points))\n",
        "control_points_reshaped = control_points.permute(2, 0, 1).unsqueeze(0)\n",
        "\n",
        "noise_grid_base = control_points_reshaped\n",
        "\n",
        "@data_poisoner\n",
        "def poison_wanet(img, label, prob=POISONING_RATE, train=True):\n",
        "    if train and (np.random.random() > prob or label != target_label):\n",
        "        return img, label\n",
        "    \n",
        "    img_tensor = torchvision.transforms.functional.pil_to_tensor(img).float() / 255.0\n",
        "    current_img_h, current_img_w = img_tensor.shape[1], img_tensor.shape[2]\n",
        "\n",
        "    # Resize noise grid to current image size\n",
        "    noise_grid = torch.nn.functional.interpolate(\n",
        "        noise_grid_base,\n",
        "        size=(current_img_h, current_img_w),\n",
        "        mode='bicubic',\n",
        "        align_corners=True\n",
        "    ).squeeze(0).permute(1, 2, 0).unsqueeze(0)\n",
        "\n",
        "    # Create identity grid and dynamic grid\n",
        "    x_coords = torch.linspace(-1, 1, current_img_w)\n",
        "    y_coords = torch.linspace(-1, 1, current_img_h)\n",
        "    grid_x, grid_y = torch.meshgrid(x_coords, y_coords, indexing='xy')\n",
        "    identity_grid = torch.stack([grid_x, grid_y], dim=-1).unsqueeze(0)\n",
        "    dynamic_grid = torch.clamp(identity_grid + s * noise_grid / current_img_h, -1, 1)\n",
        "\n",
        "    # Apply warping\n",
        "    poisoned_tensor = torch.nn.functional.grid_sample(\n",
        "        img_tensor.unsqueeze(0), dynamic_grid, align_corners=True\n",
        "    ).squeeze(0)\n",
        "\n",
        "    poisoned_array = (poisoned_tensor * 255.0).clamp(0, 255).byte()\n",
        "    if DATASET_NAME == \"MNIST\":\n",
        "        poisoned_array = poisoned_array.squeeze(0).numpy()\n",
        "    else:\n",
        "        poisoned_array = poisoned_array.permute(1, 2, 0).numpy()\n",
        "\n",
        "    poisoned_img = Image.fromarray(poisoned_array)\n",
        "\n",
        "    #for Train=True, target_label is == label otherwise would return in the if above\n",
        "    return poisoned_img, target_label\n",
        "\n",
        "\n",
        "if TRIGGER == \"square\":\n",
        "    poison = poison_square\n",
        "elif TRIGGER == \"wanet\":\n",
        "    poison = poison_wanet\n",
        "\n",
        "\n",
        "client_counts = []\n",
        "for n in range(N_POOLS):\n",
        "    for client_id in flex_miners_data[n]:\n",
        "    \n",
        "        client_dataset = flex_miners_data[n][client_id]\n",
        "        labels = [label for _, label in client_dataset]\n",
        "        count = sum(1 for lbl in labels if lbl == target_label)\n",
        "        client_counts.append((client_distributions[n][client_id], count))\n",
        "    \n",
        "client_counts.sort(key=lambda x: x[1], reverse=True)\n",
        "poisoned_global_ids = [id[0] for id in client_counts[:N_POISONED]]\n",
        "\n",
        "\n",
        "# Poison local datasets\n",
        "poisoned_clients_ids = []\n",
        "for n in range(N_POOLS):\n",
        "\n",
        "  to_poison_ids = [list(client_distributions[n]).index(gid) for gid in poisoned_global_ids if gid in client_distributions[n]]\n",
        "  \n",
        "  flex_miners_data[n] = flex_miners_data[n].apply(poison, node_ids=to_poison_ids)\n",
        "  poisoned_clients_ids.append(to_poison_ids)\n",
        "\n",
        "#Convert test dataset\n",
        "poisoned_test_data = poison(test_data, train=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def calculate_boosting_factor(poisoned_clients_ids):\n",
        "    \"\"\"\n",
        "    Calculates the boosting factor for the pool with the most poisoned clients.\n",
        "    \"\"\"\n",
        "    \n",
        "    # Check if there are any pools with poisoned clients\n",
        "    if not any(poisoned_clients_ids):\n",
        "        return 1.0\n",
        "\n",
        "    # Find the pool with the most poisoned clients\n",
        "    num_poisoned_in_pools = [len(p) for p in poisoned_clients_ids]\n",
        "    most_poisoned_pool_idx = np.argmax(num_poisoned_in_pools)\n",
        "    \n",
        "    # Calculate the boosting factor\n",
        "    boosting_factor = len(client_distributions[most_poisoned_pool_idx]) / len(poisoned_clients_ids[most_poisoned_pool_idx])\n",
        "    \n",
        "    return boosting_factor\n",
        "\n",
        "\n",
        "DEFAULT_BOOSTING = calculate_boosting_factor(poisoned_clients_ids)\n",
        "\n",
        "# MNIST dataset is very sensitive to poisoning but guarantees with minimum 1\n",
        "if DATASET_NAME == \"MNIST\":\n",
        "    DEFAULT_BOOSTING = max(1, DEFAULT_BOOSTING / 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sample = 6\n",
        "print(f\"Clean sample:\")\n",
        "print(f\"Label:\", test_data[sample][1])\n",
        "img = test_data[sample][0]\n",
        "_ = plt.imshow(img)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 432
        },
        "execution": {
          "iopub.execute_input": "2025-06-19T17:08:21.860086Z",
          "iopub.status.busy": "2025-06-19T17:08:21.859879Z",
          "iopub.status.idle": "2025-06-19T17:08:22.072412Z",
          "shell.execute_reply": "2025-06-19T17:08:22.071782Z",
          "shell.execute_reply.started": "2025-06-19T17:08:21.860063Z"
        },
        "id": "LD2IZHdNGUwg",
        "outputId": "9baf82f1-e9ea-41d4-e551-2716ab83d500",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "print(f\"Poisoned sample:\")\n",
        "print(f\"Label:\", poisoned_test_data[sample][1])\n",
        "img = poisoned_test_data[sample][0]\n",
        "_ = plt.imshow(img)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l1-pXQGqWZdh"
      },
      "source": [
        "###Rest of the code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-06-19T17:08:22.073221Z",
          "iopub.status.busy": "2025-06-19T17:08:22.073044Z",
          "iopub.status.idle": "2025-06-19T17:08:22.092273Z",
          "shell.execute_reply": "2025-06-19T17:08:22.091579Z",
          "shell.execute_reply.started": "2025-06-19T17:08:22.073206Z"
        },
        "id": "LX4QZR4Nz0-j",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def get_model(num_classes=NUM_CLASSES, model_name=MODEL_NAME):\n",
        "\n",
        "    if model_name == \"resnet18\":\n",
        "        model = resnet18(weights=ResNet18_Weights.DEFAULT)\n",
        "        model.fc = torch.nn.Linear(model.fc.in_features, num_classes)\n",
        "\n",
        "    elif model_name == \"mobilenet\":\n",
        "        model = mobilenet_v2(weights=MobileNet_V2_Weights.DEFAULT)\n",
        "        model.classifier[1] = torch.nn.Linear(model.classifier[1].in_features, num_classes)\n",
        "\n",
        "    elif model_name == \"efficientnet\":\n",
        "        model = efficientnet_b0(weights=EfficientNet_B0_Weights.DEFAULT)\n",
        "        model.classifier[1] = torch.nn.Linear(model.classifier[1].in_features, num_classes)\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "@init_server_model\n",
        "def build_server_model():\n",
        "    global server_flex_model\n",
        "\n",
        "    server_flex_model = FlexModel()\n",
        "\n",
        "    server_flex_model[\"model\"] = get_model()\n",
        "    # Required to store this for later stages of the FL training process\n",
        "    server_flex_model[\"criterion\"] = torch.nn.functional.cross_entropy\n",
        "    server_flex_model[\"optimizer_func\"] = torch.optim.Adam\n",
        "    server_flex_model[\"optimizer_kwargs\"] = {\"lr\": LR}\n",
        "\n",
        "    return server_flex_model\n",
        "\n",
        "\n",
        "def train(client_flex_model: FlexModel, client_data: Dataset):\n",
        "    train_dataset = client_data.to_torchvision_dataset(transform=data_transforms)\n",
        "    client_dataloader = DataLoader(train_dataset, batch_size=128)\n",
        "    model = client_flex_model[\"model\"]\n",
        "    optimizer = client_flex_model[\"optimizer_func\"](\n",
        "        model.parameters(), **client_flex_model[\"optimizer_kwargs\"]\n",
        "    )\n",
        "    model = model.train()\n",
        "    model = model.to(device)\n",
        "    criterion = client_flex_model[\"criterion\"]\n",
        "    for _ in range(LOCAL_EPOCHS):\n",
        "        for imgs, labels in client_dataloader:\n",
        "            imgs, labels = imgs.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            pred = model(imgs)\n",
        "            loss = criterion(pred, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "\n",
        "@collect_clients_weights\n",
        "def get_clients_weights(client_flex_model: FlexModel):\n",
        "    weight_dict = client_flex_model[\"model\"].state_dict()\n",
        "    server_dict = client_flex_model[\"server_model\"].state_dict()\n",
        "    dev = [weight_dict[name] for name in weight_dict][0].get_device()\n",
        "    dev = \"cpu\" if dev == -1 else \"cuda\"\n",
        "    return [\n",
        "        (weight_dict[name] - server_dict[name].to(dev)).type(torch.float)\n",
        "        for name in weight_dict\n",
        "    ]\n",
        "\n",
        "\n",
        "def apply_boosting(weight_list: list, coef: float):\n",
        "    set_tensorly_backend(weight_list)\n",
        "\n",
        "    n_layers = len(weight_list)\n",
        "    weights = []\n",
        "    for index_layer in range(n_layers):\n",
        "        context = tl.context(weight_list[index_layer])\n",
        "        w = weight_list[index_layer] * tl.tensor(coef, **context)\n",
        "        weights.append(w)\n",
        "    return weights\n",
        "\n",
        "\n",
        "@collect_clients_weights\n",
        "def get_poisoned_weights(client_flex_model: FlexModel):\n",
        "    weight_dict = client_flex_model[\"model\"].state_dict()\n",
        "    server_dict = client_flex_model[\"server_model\"].state_dict()\n",
        "    dev = [weight_dict[name] for name in weight_dict][0].get_device()\n",
        "    dev = \"cpu\" if dev == -1 else \"cuda\"\n",
        "    return apply_boosting(\n",
        "        [\n",
        "            (weight_dict[name] - server_dict[name].to(dev)).type(torch.float)\n",
        "            for name in weight_dict\n",
        "        ],\n",
        "        DEFAULT_BOOSTING,\n",
        "    )\n",
        "\n",
        "\n",
        "@set_aggregated_weights\n",
        "def set_agreggated_weights_to_server(server_flex_model: FlexModel, aggregated_weights):\n",
        "    dev = aggregated_weights[0].get_device()\n",
        "    dev = \"cpu\" if dev == -1 else \"cuda\"\n",
        "    with torch.no_grad():\n",
        "        weight_dict = server_flex_model[\"model\"].state_dict()\n",
        "        for layer_key, new in zip(weight_dict, aggregated_weights):\n",
        "            weight_dict[layer_key].copy_(weight_dict[layer_key].to(dev) + new)\n",
        "\n",
        "\n",
        "@deploy_server_model\n",
        "def copy_server_model_to_clients(server_flex_model: FlexModel):\n",
        "    new_flex_model = FlexModel()\n",
        "    new_flex_model[\"model\"] = copy.deepcopy(server_flex_model[\"model\"])\n",
        "    new_flex_model[\"server_model\"] = copy.deepcopy(server_flex_model[\"model\"])\n",
        "    new_flex_model[\"criterion\"] = copy.deepcopy(server_flex_model[\"criterion\"])\n",
        "    new_flex_model[\"optimizer_func\"] = copy.deepcopy(\n",
        "        server_flex_model[\"optimizer_func\"]\n",
        "    )\n",
        "    new_flex_model[\"optimizer_kwargs\"] = copy.deepcopy(\n",
        "        server_flex_model[\"optimizer_kwargs\"]\n",
        "    )\n",
        "    return new_flex_model\n",
        "\n",
        "\n",
        "def clean_up_models(clients: FlexPool):\n",
        "    import gc\n",
        "\n",
        "    clients.clients.map(lambda model, _: model.clear())\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "\n",
        "\n",
        "def obtain_metrics(server_flex_model: FlexModel, data: Dataset):\n",
        "    if data is None:\n",
        "        data = test_data\n",
        "    model = server_flex_model[\"model\"]\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    test_acc = 0\n",
        "    total_count = 0\n",
        "    model = model.to(device)\n",
        "    criterion = server_flex_model[\"criterion\"]\n",
        "    # get test data as a torchvision object\n",
        "    test_dataset = data.to_torchvision_dataset(transform=data_transforms)\n",
        "    test_dataloader = DataLoader(\n",
        "        test_dataset, batch_size=128, shuffle=False, pin_memory=False\n",
        "    )\n",
        "    losses = []\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_dataloader:\n",
        "            total_count += target.size(0)\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            losses.append(criterion(output, target).item())\n",
        "            pred = output.data.max(1, keepdim=True)[1]\n",
        "            test_acc += pred.eq(target.data.view_as(pred)).long().cpu().sum().item()\n",
        "\n",
        "    test_loss = sum(losses) / len(losses)\n",
        "    test_acc /= total_count\n",
        "    return test_loss, test_acc\n",
        "\n",
        "\n",
        "\n",
        "def obtain_accuracy(server_flex_model: FlexModel, data: Dataset):\n",
        "    return obtain_metrics(server_flex_model, data)\n",
        "\n",
        "def obtain_backdoor_metrics(server_flex_model: FlexModel, _):\n",
        "    return obtain_metrics(server_flex_model, poisoned_test_data)\n",
        "\n",
        "\n",
        "def obtain_eval_metrics(server_flex_model: FlexModel, _):\n",
        "    return obtain_metrics(server_flex_model, test_data)\n",
        "\n",
        "\n",
        "def warmup(pool: FlexPool, poisoned_clients_ids):\n",
        "\n",
        "    clean_clients = pool.clients.select(\n",
        "        lambda client_id, _: client_id not in poisoned_clients_ids\n",
        "    )\n",
        "\n",
        "    #Just epochs on benign clients to warm up the model\n",
        "    for i in tqdm(range(5), \"WARMUP BASE\"):\n",
        "        pool.servers.map(copy_server_model_to_clients, clean_clients)\n",
        "\n",
        "        clean_clients.map(train)\n",
        "\n",
        "        pool.aggregators.map(get_clients_weights, clean_clients)\n",
        "        pool.aggregators.map(fed_avg)\n",
        "        pool.aggregators.map(set_agreggated_weights_to_server, pool.servers)\n",
        "\n",
        "        clean_up_models(clean_clients)\n",
        "\n",
        "    acc = pool.servers.map(obtain_eval_metrics)\n",
        "    loss, accuracy = acc[0]\n",
        "    print(\"Warmup Accuracy:\")\n",
        "    print(f\"  - Loss     : {loss:.4f}\")\n",
        "    print(f\"  - Accuracy : {accuracy * 100:.2f}%\")\n",
        "\n",
        "    return acc\n",
        "\n",
        "\n",
        "def train_round(pool: FlexPool, poisoned_clients_ids):\n",
        "\n",
        "    poisoned_clients = pool.clients.select(\n",
        "        lambda client_id, _: client_id in poisoned_clients_ids\n",
        "    )\n",
        "    clean_clients = pool.clients.select(\n",
        "        lambda client_id, _: client_id not in poisoned_clients_ids\n",
        "    )\n",
        "\n",
        "\n",
        "    pool.servers.map(copy_server_model_to_clients, clean_clients)\n",
        "    pool.servers.map(copy_server_model_to_clients, poisoned_clients)\n",
        "\n",
        "    clean_clients.map(train)\n",
        "    poisoned_clients.map(train)\n",
        "\n",
        "    pool.aggregators.map(get_clients_weights, clean_clients)\n",
        "    pool.aggregators.map(get_poisoned_weights, poisoned_clients)\n",
        "\n",
        "    pool.aggregators.map(fed_avg)\n",
        "    pool.aggregators.map(set_agreggated_weights_to_server, pool.servers)\n",
        "\n",
        "    clean_up_models(clean_clients)\n",
        "    clean_up_models(poisoned_clients)\n",
        "\n",
        "\n",
        "    main_acc = pool.servers.map(obtain_eval_metrics)\n",
        "    backdoor_acc = pool.servers.map(obtain_backdoor_metrics)\n",
        "\n",
        "    return main_acc, backdoor_acc\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-06-19T17:08:22.093174Z",
          "iopub.status.busy": "2025-06-19T17:08:22.092945Z",
          "iopub.status.idle": "2025-06-19T17:08:22.114875Z",
          "shell.execute_reply": "2025-06-19T17:08:22.114164Z",
          "shell.execute_reply.started": "2025-06-19T17:08:22.093158Z"
        },
        "id": "_LhkfkrrvezW",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "BETA = 0.7\n",
        "reputation = [0.0 for _ in range(N_POOLS)]\n",
        "\n",
        "def por_consensus_mechanism(pools, eval_function, eval_dataset, reputation, beta=BETA):\n",
        "    \"\"\"\n",
        "    Implement the PoR consensus mechanism for selecting a miner based on model reputation.\n",
        "    \"\"\"\n",
        "\n",
        "    for idx, pool in enumerate(pools):\n",
        "        model = pool.aggregators._models['server']\n",
        "        loss, acc = eval_function(model, eval_dataset)\n",
        "\n",
        "        # validation accuracy and inverse loss\n",
        "        score = acc + (1 / (1 + loss))\n",
        "\n",
        "        # Update the reputation\n",
        "        reputation[idx] = beta * reputation[idx] + (1 - beta) * score\n",
        "\n",
        "        print(f\"[PoR] Pool: {idx} - New Reputation: {reputation[idx]:.4f}\")\n",
        "\n",
        "\n",
        "    winner = int(np.argmax(reputation))\n",
        "    print(f\"[PoR] Winer Pool: {winner} Best Reputation: {reputation[winner]}\")\n",
        "    return winner, reputation\n",
        "\n",
        "\n",
        "def update_blockchain(pools, blockchain, reputation):\n",
        "    \"\"\"\n",
        "    Achieve consensus between all pools, update blockchain with a new block\n",
        "    and propagate winner weights to all other pools\n",
        "    \"\"\"\n",
        "\n",
        "    # Get winner through PoW consensus\n",
        "    winner, reputation = por_consensus_mechanism(pools, obtain_accuracy, val_data, reputation)\n",
        "    winner_weights = pools[winner].aggregators._models['server']['model'].state_dict()\n",
        "\n",
        "    new_block = BlockPoFL(weights=[param.cpu().numpy() for param in winner_weights.values()]) #convert dict to list as block only accepts that type\n",
        "    blockchain.add_block(new_block)\n",
        "\n",
        "    #Propagate winner weights to other pools\n",
        "    for i, pool in enumerate(pools):\n",
        "        if i != winner:\n",
        "            pool.aggregators._models['server']['model'].load_state_dict(winner_weights)\n",
        "\n",
        "    return winner, reputation\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2025-06-19T17:08:22.115850Z",
          "iopub.status.busy": "2025-06-19T17:08:22.115631Z",
          "iopub.status.idle": "2025-06-19T18:42:44.150434Z",
          "shell.execute_reply": "2025-06-19T18:42:44.149862Z",
          "shell.execute_reply.started": "2025-06-19T17:08:22.115834Z"
        },
        "id": "SGNrf_6KvpbD",
        "outputId": "b46b2f34-d46e-4944-8f3b-0d4eeea87817",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "blockchain = BlockchainPoFL(BlockPoFL([]))  # Genesis block is created with difficulty=3\n",
        "pools = []\n",
        "\n",
        "print(f\"***Proof-of-Reputation Experiment***\")\n",
        "print(f\"  Model   : {MODEL_NAME}\")\n",
        "print(f\"  Dataset : {DATASET_NAME}\")\n",
        "print(f\"  Non-IID : {NON_IID_RATE}\")\n",
        "print(f\"  Pools   : {N_POOLS}\")\n",
        "print(f\"  Trigger : {TRIGGER}\")\n",
        "\n",
        "\n",
        "print(\"\\n\" + \"=\"*40)\n",
        "print(\"      POISONED CLIENTS PER POOL\")\n",
        "print(\"=\"*40)\n",
        "for i, poisoned in enumerate(poisoned_clients_ids):\n",
        "    print(f\"Miner {i}: {len(poisoned)}\")\n",
        "print(\"=\"*40 + \"\\n\")\n",
        "\n",
        "\n",
        "#WARMUP\n",
        "print(\"\\n\" + \"=\"*40)\n",
        "print(\"               WARMING UP\")\n",
        "print(\"=\"*40)\n",
        "for n in range(N_POOLS):\n",
        "    print(f\"       *** [MINER {n}] ***\")\n",
        "    pool = FlexPool.client_server_pool(flex_miners_data[n], build_server_model)\n",
        "    acc = warmup(pool, poisoned_clients_ids[n])\n",
        "    pools.append(pool)\n",
        "\n",
        "winner, _ = update_blockchain(pools, blockchain, reputation)\n",
        "\n",
        "# Resets reputations after warmup\n",
        "reputation = [0.0 for _ in range(N_POOLS)]\n",
        "\n",
        "\n",
        "#TRAINING ROUNDS\n",
        "for i in tqdm(range(N_ROUNDS), \"ROUNDS OF TRAINING\"):\n",
        "    print(\"\\n\" + \"=\"*40)\n",
        "    print(f\"            TRAINING ROUND {i}\")\n",
        "    print(\"=\"*40)\n",
        "\n",
        "    # Train all pools\n",
        "    for n in range(N_POOLS):\n",
        "        print(f\"\\n{'*'*30}\")\n",
        "        print(f\"       *** [MINER {n}] ***\")\n",
        "\n",
        "        metrics, backdoor_metrics = train_round(pools[n], poisoned_clients_ids[n])\n",
        "\n",
        "        loss, accuracy = metrics[0]\n",
        "        bd_loss, bd_accuracy = backdoor_metrics[0]\n",
        "\n",
        "        print(f\"Clean Data   :  Loss: {loss:.4f} | Acc: {accuracy * 100:.2f}%\")\n",
        "        print(f\"Backdoor Data:  Loss: {bd_loss:.4f} | Acc: {bd_accuracy * 100:.2f}%\")\n",
        "\n",
        "        print(f\"{'*'*30}\")\n",
        "\n",
        "    winner, reputation = update_blockchain(pools, blockchain, reputation)\n",
        "\n",
        "loss, accuracy = pools[winner].servers.map(obtain_eval_metrics)[0]\n",
        "bd_loss, bd_accuracy = pools[winner].servers.map(obtain_backdoor_metrics)[0]\n",
        "\n",
        "print(f\"{'='*30}\")\n",
        "print(f\" *** Final Model Metrics ***\")\n",
        "\n",
        "\n",
        "print(\"Clean Data:\")\n",
        "print(f\"  - Loss     : {loss:.4f}\")\n",
        "print(f\"  - Accuracy : {accuracy * 100:.2f}%\")\n",
        "\n",
        "print(\"Backdoor Data:\")\n",
        "print(f\"  - Loss     : {bd_loss:.4f}\")\n",
        "print(f\"  - Accuracy : {bd_accuracy * 100:.2f}%\")\n",
        "\n",
        "print(f\"{'='*30}\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [],
      "dockerImageVersionId": 31040,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "jupyter-env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
